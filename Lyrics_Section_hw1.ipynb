{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91c8969e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Twitter API Pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "185076b0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# for the twitter section\n",
    "import tweepy\n",
    "import os\n",
    "import datetime\n",
    "import re\n",
    "from pprint import pprint\n",
    "\n",
    "# for the lyrics scrape section\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a47e2d5f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "def timer(length_of_time):\n",
    "    timer = round(length_of_time / 60)\n",
    "\n",
    "    for num in range(0,timer,1):\n",
    "        time.sleep(60)\n",
    "        print(f'Sequence {num} complete out of {timer}.')\n",
    "        \n",
    "    print('Ring Ring Ring')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e9e648-c04a-4cbb-8b09-2a5c7c7c06e5",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Webpage Specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3876dbaa-b1fb-4e95-a024-b017323834d5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def nap_time(print_=False):\n",
    "    length = 5 + 10*random.random()\n",
    "    \n",
    "    if print_:\n",
    "        print(f'Snoring for {length} seconds')\n",
    "    time.sleep(length)\n",
    "    \n",
    "\n",
    "def folder_maker(folder2make):\n",
    "    # create directory to store html files\n",
    "    if os.path.isdir(folder2make):\n",
    "        shutil.rmtree(folder2make)\n",
    "    os.mkdir(folder2make)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c13af3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "\n",
    "# Lyrics Scrape\n",
    "\n",
    "This section asks you to pull data from the Twitter API and scrape www.AZLyrics.com. In the notebooks where you do that work you are asked to store the data in specific ways. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bd7df77",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "artists = {'FFDP':\"https://www.azlyrics.com/f/fivefingerdeathpunch.html\",\n",
    "           'OfficialRezz':\"https://www.azlyrics.com/r/rezz.html\"} \n",
    "# we'll use this dictionary to hold both the artist name and the link on AZlyrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c236c99b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## A Note on Rate Limiting\n",
    "\n",
    "The lyrics site, www.azlyrics.com, does not have an explicit maximum on number of requests in any one time, but in our testing it appears that too many requests in too short a time will cause the site to stop returning lyrics pages. (Entertainingly, the page that gets returned seems to only have the song title to [a Tom Jones song](https://www.azlyrics.com/lyrics/tomjones/itsnotunusual.html).) \n",
    "\n",
    "Whenever you call `requests.get` to retrieve a page, put a `time.sleep(5 + 10*random.random())` on the next line. This will help you not to get blocked. If you _do_ get blocked, which you can identify if the returned pages are not correct, just request a lyrics page through your browser. You'll be asked to perform a CAPTCHA and then your requests should start working again. \n",
    "\n",
    "## Part 1: Finding Links to Songs Lyrics\n",
    "\n",
    "That general artist page has a list of all songs for that artist with links to the individual song pages. \n",
    "\n",
    "Q: Take a look at the `robots.txt` page on www.azlyrics.com. (You can read more about these pages [here](https://developers.google.com/search/docs/advanced/robots/intro).) Is the scraping we are about to do allowed or disallowed by this page? How do you know? \n",
    "\n",
    "A: The website source code has no reference to a robots.txt file, nor does a Google Search bring up such a document.  After reading the site's privacy policy page as well as other irrelevant pages, it appears AZLyrics implicitly allows bot crawling and scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac9d31ec",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Let's set up a dictionary of lists to hold our links\n",
    "lyrics_pages = defaultdict(list)\n",
    "\n",
    "# create lyrics folder\n",
    "folder_maker('lyrics')\n",
    "\n",
    "for num, (artist, artist_page) in enumerate(artists.items()):\n",
    "    # request the page and sleep\n",
    "    r = requests.get(artist_page)\n",
    "    nap_time()\n",
    "    \n",
    "    # create individual artist folder\n",
    "    folder_maker(f'lyrics/{artist}')\n",
    "\n",
    "    # check to see if request worked\n",
    "    if r.ok:\n",
    "        file_name = f'lyrics/{artist}/{artist}_main_page.html'\n",
    "        \n",
    "        # save/write contents of artist's main page\n",
    "        with open(file_name, 'w+') as f:\n",
    "            f.write(r.text)\n",
    "            f.close()\n",
    "\n",
    "        soup = BeautifulSoup(r.text.encode('utf-8'), 'html.parser')\n",
    "\n",
    "        song_pages = soup.find_all('div', id='listAlbum')[0].find_all('div', class_='listalbum-item')\n",
    "\n",
    "        lyric_links = []\n",
    "        for page in song_pages:\n",
    "            try:\n",
    "                lyric_links.append('azlyrics.com'+page.a['href'])\n",
    "            except TypeError:\n",
    "                # TypeError occurs when song has no lyrics, i.e. instrumental\n",
    "                pass\n",
    "\n",
    "        lyrics_pages[f'{artist}'] = lyric_links\n",
    "        \n",
    "        pd.DataFrame({f'{artist}':lyric_links}).to_csv(f'lyrics/{artist}/{artist}_lyric_pages.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c285ec1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's make sure we have enough lyrics pages to scrape. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da78ee0b-d485-4054-a8f4-38a6801d5f5e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# df1 = pd.read_csv('htmls/cher_lyric_pages.csv').to_dict()\n",
    "# df2 = pd.read_csv('htmls/robyn_lyric_pages.csv').to_dict()\n",
    "\n",
    "# # merge the two dictionaries back into 1\n",
    "# lyrics_pages = df1 | df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae4cda68",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for artist, lp in lyrics_pages.items() :\n",
    "    assert(len(set(lp)) > 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edca10d6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For FFDP we have 125.\n",
      "The full pull will take for this artist will take 0.35 hours.\n",
      "For OfficialRezz we have 23.\n",
      "The full pull will take for this artist will take 0.06 hours.\n"
     ]
    }
   ],
   "source": [
    "# Let's see how long it's going to take to pull these lyrics \n",
    "# if we're waiting `5 + 10*random.random()` seconds \n",
    "for artist, links in lyrics_pages.items() : \n",
    "    print(f\"For {artist} we have {len(links)}.\")\n",
    "    print(f\"The full pull will take for this artist will take {round(len(links)*10/3600,2)} hours.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011be6c6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Part 2: Pulling Lyrics\n",
    "\n",
    "Now that we have the links to our lyrics pages, let's go scrape them! Here are the steps for this part. \n",
    "\n",
    "1. Create an empty folder in our repo called \"lyrics\". \n",
    "1. Iterate over the artists in `lyrics_pages`. \n",
    "1. Create a subfolder in lyrics with the artist's name. For instance, if the artist was Cher you'd have `lyrics/cher/` in your repo.\n",
    "1. Iterate over the pages. \n",
    "1. Request the page and extract the lyrics from the returned HTML file using BeautifulSoup.\n",
    "1. Use the function below, `generate_filename_from_url`, to create a filename based on the lyrics page, then write the lyrics to a text file with that name. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb648af7-1b15-48d8-88f1-5b47acb2e6fa",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def lyrics_html2text(text):\n",
    "    text = re.sub(r'\\n', ' ',text)\n",
    "    text = re.sub(r'\\r', ' ',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d655b687",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "match_ = '<div>\\n<!-- Usage'\n",
    "schema = \"https://www.\" \n",
    "start = time.time()\n",
    "\n",
    "total_pages = 0 \n",
    "\n",
    "# lyrics_list = []\n",
    "for artist in lyrics_pages :\n",
    "    links = lyrics_pages[artist]\n",
    "    \n",
    "    lyrics_list = []\n",
    "    # 2. Iterate over the lyrics pages\n",
    "    for link in links[:20]:\n",
    "        nap_time()\n",
    "\n",
    "        r = requests.get(schema+link)\n",
    "        \n",
    "        if r.ok:\n",
    "            \n",
    "            soup = BeautifulSoup(r.text.encode('utf-8'), 'html.parser')\n",
    "            \n",
    "            # html elements\n",
    "            elements = soup.find_all('div', class_='col-xs-12 col-lg-8 text-center')[0]\n",
    "\n",
    "            # seek title\n",
    "            song_title = elements.find_all('b')[1].text.replace('\"','')\n",
    "            song_title = song_title.replace(' ','_') \n",
    "\n",
    "            for element in elements:\n",
    "                element_string = str(element)[:16]\n",
    "\n",
    "                if element_string == match_:\n",
    "                    lyrics = element.get_text()\n",
    "                    lyrics = lyrics_html2text(lyrics)\n",
    "\n",
    "                    song_dict = {'Artist':artist,\n",
    "                                 'Title':song_title,\n",
    "                                 'Lyrics':lyrics}\n",
    "\n",
    "                    lyrics_list.append(song_dict)\n",
    "                    \n",
    "                    # save/write lyrics to independent file\n",
    "                    with open(f'lyrics/{artist}/{song_title}' + '.txt', 'w+') as f:\n",
    "                        f.write(lyrics)\n",
    "                        f.close()\n",
    "\n",
    "    # write all of THE artist data into a pandas DF schema csv file\n",
    "    pd.DataFrame(lyrics_list).to_csv(f'lyrics/{artist}/{artist}_song_lyrics_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54fddfd-f231-4236-bd80-653b68d09abc",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36c394f1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total run time was 0.13 hours.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total run time was {round((time.time() - start)/3600,2)} hours.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054cf14b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "This assignment asks you to pull data from the Twitter API and scrape www.AZLyrics.com.  After you have finished the above sections , run all the cells in this notebook. Print this to PDF and submit it, per the instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "217c2b0a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Simple word extractor from Peter Norvig: https://norvig.com/spell-correct.html\n",
    "def words(text): \n",
    "    return re.findall(r'\\w+', text.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37778a1c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Checking Lyrics \n",
    "\n",
    "The output from your lyrics scrape should be stored in files located in this path from the directory:\n",
    "`/lyrics/[Artist Name]/[filename from URL]`. This code summarizes the information at a high level to help the instructor evaluate your work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bccac29c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For FFDP we have 22 files.\n",
      "For FFDP we have roughly 9712 words, 1094 are unique.\n",
      "For OfficialRezz we have 22 files.\n",
      "For OfficialRezz we have roughly 4993 words, 564 are unique.\n"
     ]
    }
   ],
   "source": [
    "artist_folders = os.listdir(\"lyrics/\")\n",
    "artist_folders = [f for f in artist_folders if os.path.isdir(\"lyrics/\" + f)]\n",
    "\n",
    "for artist in artist_folders : \n",
    "    artist_files = os.listdir(\"lyrics/\" + artist)\n",
    "    artist_files = [f for f in artist_files if 'txt' in f or 'csv' in f or 'tsv' in f]\n",
    "\n",
    "    print(f\"For {artist} we have {len(artist_files)} files.\")\n",
    "\n",
    "    artist_words = []\n",
    "\n",
    "    for f_name in artist_files : \n",
    "        with open(\"lyrics/\" + artist + \"/\" + f_name) as infile : \n",
    "            artist_words.extend(words(infile.read()))\n",
    "\n",
    "            \n",
    "    print(f\"For {artist} we have roughly {len(artist_words)} words, {len(set(artist_words))} are unique.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}